{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SkigGram.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejbKT0p4tIVB",
        "outputId": "44dcabfd-2411-4b26-9bf1-8add617feb8e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.insert(0,'/content/drive/My Drive/python_modules')"
      ],
      "metadata": {
        "id": "Tpu9A-F1tGYk"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "m4180LWKua57"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "-94LFgJE7UGr"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from NLP.Utils import Utils"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **SkipGram Model**"
      ],
      "metadata": {
        "id": "junvaxJ-tTla"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SkipGram:\n",
        "    def __init__(self):\n",
        "        self.learning_rate = 0.01\n",
        "\n",
        "    # preprocess corpus\n",
        "    def clean_corpus(self, corpus):\n",
        "        if type(corpus[0]) == list:\n",
        "            clean_corpus, word_counts = Utils.clean_docs(corpus)\n",
        "            return clean_corpus, word_counts\n",
        "        else:\n",
        "            prep_corpus = Utils.tokenize(corpus, 'bulk')\n",
        "            clean_corpus, word_counts = Utils.clean_docs(prep_corpus)\n",
        "            return clean_corpus, word_counts\n",
        "        \n",
        "    # convert output to probability\n",
        "    def softmax(self, u):\n",
        "        x = np.exp(u) / np.sum(np.exp(u))\n",
        "        return x\n",
        "    \n",
        "    # initialize words weights\n",
        "    def initialize_weights(self, V, N):\n",
        "        \"\"\"\n",
        "            W1 (hidden layer) = shape(Vocab_length(V), Word_dim(N))\n",
        "            W1 = | d1-w1 d2-w1 ... dN-w1 |\n",
        "                 | d1-w2 d2-w2 ... dN-w2 |\n",
        "                 | d1-wV d2-wV ... dN-wV |\n",
        "\n",
        "            W2 (output layer) = shape(Word_dim(N), Vocab_length(V))\n",
        "            W2 = | d1-w1 d1-w2 ... d1-wV |\n",
        "                 | d2-w1 d2-w2 ... d2-wV |\n",
        "                 | dN-w1 dN-w2 ... dN-wV |\n",
        "        \"\"\"\n",
        "        np.random.seed(0)\n",
        "        self.W1 = np.random.randn(V, N).astype('float128')\n",
        "        self.W2 = np.random.randn(N, V).astype('float128') \n",
        "    \n",
        "    # update weights based on gradient\n",
        "    def update_weights(self, dW2, dW1):\n",
        "        self.W2 = self.W2 - self.learning_rate * dW2\n",
        "        self.W1 = self.W1 - self.learning_rate * dW1\n",
        "\n",
        "    # feed forward\n",
        "    def forward(self, X, predict=False, k=5):\n",
        "        \"\"\"\n",
        "            X = [0, 0, 1, ... , V].T -> shape(V, 1)\n",
        "            h (hidden layer) = W1.T @ X -> (N, V) @ (V, 1) = (N, 1)\n",
        "            u (output layer) = W2.T @ h -> (V, N) @ (N, 1) = (V, 1)\n",
        "            y (output prob) = softmax(u)\n",
        "        \"\"\"\n",
        "\n",
        "        self.h = np.dot(self.W1.T, X)\n",
        "        self.u = np.dot(self.W2.T, self.h)\n",
        "        self.y = self.softmax(self.u)\n",
        "\n",
        "        # if trying to predict most similar words\n",
        "        if predict:\n",
        "            words = []\n",
        "            # store pred and keep trach of their index\n",
        "            pred = dict(zip(range(len(self.y)), self.y))\n",
        "            # sort based on probability of each word to be a context word\n",
        "            pred_sorted = sorted(pred, key=lambda x: pred[x], reverse=True)\n",
        "            # select the top k words\n",
        "            top_context = pred_sorted[:k]\n",
        "            # grab the word using its index from the vocab\n",
        "            for w in top_context:\n",
        "                words.append(self.vocab[w])\n",
        "\n",
        "            return words\n",
        "\n",
        "    # backprop error and calculate gradient\n",
        "    def backprop(self, x, label):\n",
        "        \"\"\"\n",
        "            error = (pred - true) -> (V, 1)\n",
        "            dW2 = dE/dy * dy/dW2 = h @ error.T -> (N, 1) @ (1, V) -> (N, V)(=W2 shape)\n",
        "            dh = W2 @ error -> backpropagate error to the hidden layer -> (N, 1)\n",
        "            dW1 = x @ dh.T -> outer product (V, 1) @ (1, N) -> (V, N) (=W1 shape)\n",
        "        \"\"\"\n",
        "        error = self.y - label\n",
        "        dW2 = np.dot(self.h, error.T)\n",
        "        dh = np.dot(self.W2, error)\n",
        "        dW1 = np.dot(x, dh.T)\n",
        "        self.update_weights(dW2, dW1)\n",
        "\n",
        "\n",
        "    # train the model\n",
        "    def fit(self, corpus, N=200, window_size=2, epochs=500, learning_rate=.01):\n",
        "        # clean corpus\n",
        "        corpus, word_counts = self.clean_corpus(corpus)\n",
        "        self.vocab = sorted(list(word_counts.keys()))\n",
        "        self.word_index = Utils.vocab_idx(self.vocab)\n",
        "\n",
        "        # initialize parameters\n",
        "        self.initialize_weights(len(self.vocab), N)\n",
        "\n",
        "        if learning_rate is not None:\n",
        "            self.learning_rate = learning_rate\n",
        "        \n",
        "        self.train_loss = []\n",
        "        vocab_len = len(self.vocab)\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            loss = 0\n",
        "            for doc in corpus:\n",
        "                # index to track position in the doc\n",
        "                current_index = 0\n",
        "                \n",
        "                doc_len = len(doc)\n",
        "                # grab center word context words\n",
        "                while current_index < doc_len:\n",
        "                    # center word\n",
        "                    word = doc[current_index]\n",
        "\n",
        "                    # words in window size\n",
        "                    left_window = max(0, current_index - window_size)\n",
        "                    right_window= min(current_index + window_size, doc_len)\n",
        "                    context_words = doc[left_window:current_index] + doc[current_index+1: right_window]\n",
        "                    \n",
        "                    # prepare label (index of context words)\n",
        "                    label = np.zeros((vocab_len, 1))\n",
        "\n",
        "                    for cw in context_words:\n",
        "                        cw_idx = self.word_index[cw]\n",
        "                        # change value of label to 1 for words in context\n",
        "                        label[cw_idx] = 1\n",
        "                    \n",
        "                    # convert input word to one-hot-vector\n",
        "                    center_word_idx = self.word_index[word]\n",
        "                    center_word_vector = np.zeros((vocab_len, 1))\n",
        "                    center_word_vector[center_word_idx] = 1\n",
        "\n",
        "                    # feed forward word through network\n",
        "                    self.forward(center_word_vector) \n",
        "\n",
        "                    # backprop\n",
        "                    self.backprop(center_word_vector, label)\n",
        "\n",
        "                    # calculate loss\n",
        "                    loss = -np.sum(self.u[label==1]) + len(context_words) * np.log(sum(np.exp(self.u)))\n",
        "\n",
        "                    # increment index\n",
        "                    current_index += 1 \n",
        "\n",
        "            self.train_loss.append((epoch, loss))\n",
        "            if epoch > 1:\n",
        "                if abs(self.train_loss[-2][1][0] - self.train_loss[-1][1][0]) < 1e-4 \\\n",
        "                or self.train_loss[-2][1][0] < self.train_loss[-1][1][0]:\n",
        "                    print ('Stopping Early')\n",
        "                    break\n",
        "\n",
        "            if epoch % 10 == 0:\n",
        "                print(\"Epoch: \", epoch, \"Loss\", loss)\n",
        "\n",
        "    # make predictions\n",
        "    def predict(self, x, k=5):\n",
        "        # convert word to one-hot-vector\n",
        "        center_word_idx = self.word_index[x.lower()]\n",
        "        center_word_vector = np.zeros((len(self.vocab), 1))\n",
        "        center_word_vector[center_word_idx] = 1\n",
        "        # feed it forward in the network and grab most similar words\n",
        "        return self.forward(center_word_vector, predict=True, k=k)"
      ],
      "metadata": {
        "id": "SsjEDfSqGqOA"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing Model"
      ],
      "metadata": {
        "id": "B1J2OklOtZfX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simple Test"
      ],
      "metadata": {
        "id": "COYQupP29T7u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "simple_corpus = \"The quick brown fox jumps over the lazy dog.\""
      ],
      "metadata": {
        "id": "C981Ls4s9eYh"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "simple_sg = SkipGram()\n",
        "simple_sg.fit(simple_corpus, N=10, window_size=2, epochs=100, learning_rate=.01)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5cJFQ8_T9egM",
        "outputId": "9bc06b51-1604-4648-82e1-51f2369f2747"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:  0 Loss [4.5641709]\n",
            "Epoch:  10 Loss [2.53427398]\n",
            "Epoch:  20 Loss [1.80073308]\n",
            "Epoch:  30 Loss [1.53672327]\n",
            "Epoch:  40 Loss [1.44471628]\n",
            "Epoch:  50 Loss [1.42416028]\n",
            "Stopping Early\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "simple_sg.predict('fox', 2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9cE1qoGE-B_i",
        "outputId": "ee7c6d06-a9e0-4391-e865-330e553153c8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['brown', 'jumps']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using treebank corpus"
      ],
      "metadata": {
        "id": "7isGqCMh9ZFp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Grab random 100 doc from tree bank corpus"
      ],
      "metadata": {
        "id": "3iTA6taduN3o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('treebank')\n",
        "from nltk.corpus import treebank"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tZsxl4CfRHZ8",
        "outputId": "bb16b404-baac-486e-ef12-e67c08c6d874"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Package treebank is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(0)\n",
        "corpus = np.random.choice(treebank.sents(), 100)"
      ],
      "metadata": {
        "id": "3Rdqm4_bS6lY"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tree_bank_sg = SkipGram()\n",
        "tree_bank_sg.fit(corpus, N=50, window_size=3, epochs=50, learning_rate=.01)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PVsaHhViIsZ2",
        "outputId": "26f4b4cf-98a7-46c4-81c4-6a5238db6b77"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:  0 Loss [20.40035635]\n",
            "Epoch:  10 Loss [5.93900068]\n",
            "Epoch:  20 Loss [0.59148718]\n",
            "Stopping Early\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.random.choice(corpus))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88DHTj9BbS7C",
        "outputId": "d984d29f-8d7c-4545-8639-a950c526f6f8"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['But', 'the', 'strength', 'in', 'heating', 'oil', 'helped', '*-1', 'push', 'up', 'crude', 'oil', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tree_bank_sg.predict('oil')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q3euuVbObS_s",
        "outputId": "200f603d-6756-402a-b365-cf4353b7e5d4"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['helped', 'push', 'crude', 'strength', 'spending']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    }
  ]
}