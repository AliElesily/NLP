{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SkipGram_Negative_sampling.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ymkQL93Jl-z2",
        "outputId": "bdf80f14-6385-4b21-a57c-208d85e89123"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.insert(0,'/content/drive/My Drive/python_modules')"
      ],
      "metadata": {
        "id": "Upu99eoll_rE"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "0aVeBO4nIRbr"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from NLP.Utils import Utils"
      ],
      "metadata": {
        "id": "61dp5uSwmF5k"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OqNvUbGek5-h",
        "outputId": "d5720a84-16dc-418d-bbde-01022cf12af7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SkipGram:\n",
        "    def __init__(self):\n",
        "        self.learning_rate = 0.001\n",
        "\n",
        "    def softmax(self, u):\n",
        "        return np.exp(u) / np.sum(np.exp(u))\n",
        "\n",
        "    # convert output to probability\n",
        "    # sigmoid for neg sampling\n",
        "    def sigmoid(self, u):\n",
        "        out = 1 / (1 + np.exp(-u))\n",
        "        return out\n",
        "\n",
        "    # preprocess corpus\n",
        "    def clean_corpus(self, corpus):\n",
        "        if type(corpus[0]) == list:\n",
        "            clean_corpus, word_counts = Utils.clean_docs(corpus)\n",
        "            return clean_corpus, word_counts\n",
        "        else:\n",
        "            prep_corpus = Utils.tokenize(corpus, 'bulk')\n",
        "            clean_corpus, word_counts = Utils.clean_docs(prep_corpus)\n",
        "            return clean_corpus, word_counts\n",
        "    \n",
        "    # Unigram\n",
        "    def create_unigram(self, word_counts):\n",
        "        self.word_prob = Utils.unigram(word_counts)\n",
        "    \n",
        "    # grab negative samples\n",
        "    def negative_sample(self, k=10):\n",
        "        samples = Utils.negative_sample(self.word_prob, k)\n",
        "        samples_idx = [self.word_index[s] for s in samples]\n",
        "        return samples_idx\n",
        "    \n",
        "    # initialize words weights\n",
        "    def initialize_weights(self, V, N):\n",
        "        \"\"\"\n",
        "            W1 (hidden layer) = shape(Vocab_length(V), Word_dim(N))\n",
        "            W1 = | d1-w1 d2-w1 ... dN-w1 |\n",
        "                 | d1-w2 d2-w2 ... dN-w2 |\n",
        "                 | d1-wV d2-wV ... dN-wV |\n",
        "\n",
        "            W2 (output layer) = shape(Word_dim(N), Vocab_length(V))\n",
        "            W2 = | d1-w1 d1-w2 ... d1-wV |\n",
        "                 | d2-w1 d2-w2 ... d2-wV |\n",
        "                 | dN-w1 dN-w2 ... dN-wV |\n",
        "        \"\"\"\n",
        "        np.random.seed(0)\n",
        "        self.W1 = np.random.randn(V, N).astype('float128')\n",
        "        self.W2 = np.random.randn(N, V).astype('float128')\n",
        "\n",
        "\n",
        "    # update weights based on gradient\n",
        "    def update_weights(self, dW2, dW1, neg_sample, cwidx):\n",
        "        self.W2[:, neg_sample] = self.W2[:, neg_sample] - self.learning_rate * dW2\n",
        "        self.W1[cwidx, :] = self.W1[cwidx, :] - self.learning_rate * dW1\n",
        "\n",
        "    # feed forward\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "            X = [0, 0, 1, ... , V].T -> shape(V, 1)\n",
        "            h (hidden layer) = W1.T @ X -> (N, V) @ (V, 1) = (N, 1)\n",
        "            u (output layer) = W2.T @ h -> (V, N) @ (N, 1) = (V, 1)\n",
        "            y (output prob) = softmax(u)\n",
        "        \"\"\"\n",
        "\n",
        "        self.h = np.dot(self.W1.T, X)\n",
        "        self.u = np.dot(self.W2.T, self.h)\n",
        "        self.y = self.sigmoid(self.u)\n",
        "\n",
        "    # backprop error and calculate gradient\n",
        "    def backprop(self, x, label, samples, center_word_idx):\n",
        "        \"\"\"\n",
        "            calculate error only with respect to chosen samples\n",
        "            error = (pred[samples] - true[samples]) -> (V[samples], 1)\n",
        "            dW2 = dE/dy * dy/dW2 = h @ error.T -> (N, 1) @ (1, V[samples]) -> (N, V[samples])\n",
        "            dh = W2[samples] @ error -> backpropagate error to the hidden layer -> (N, 1)\n",
        "            dW1 = df.T -> (1, N) x is one-hot-vector so only input word will be updated\n",
        "        \"\"\"\n",
        "        # calculate error from selected samples\n",
        "        error = self.y[samples] - label[samples]\n",
        "        # grad w.r.t W2\n",
        "        dW2 = np.dot(self.h, error.T)\n",
        "        # backpropagate error from outputlayer to hidden layer\n",
        "        dh = np.dot(self.W2[:, samples], error)\n",
        "        # grad w.r.t W1\n",
        "        dW1 = dh.reshape(1, -1)\n",
        "        # update weights\n",
        "        self.update_weights(dW2, dW1, samples, center_word_idx)\n",
        "\n",
        "    # train the model\n",
        "    def fit(self, corpus, N=300, window_size=2, epochs=500, learning_rate=None):\n",
        "        # clean corpus\n",
        "        corpus, word_counts = self.clean_corpus(corpus)\n",
        "        self.vocab = sorted(list(word_counts.keys()))\n",
        "        self.word_index = Utils.vocab_idx(self.vocab)\n",
        "\n",
        "        # generate unigram\n",
        "        self.create_unigram(word_counts)\n",
        "\n",
        "        # initialize parameters\n",
        "        self.initialize_weights(len(self.vocab), N)\n",
        "\n",
        "        if learning_rate is not None:\n",
        "            self.learning_rate = learning_rate\n",
        "        \n",
        "        self.train_loss = []\n",
        "        # iterateive process to train model\n",
        "        for epoch in range(epochs):\n",
        "            loss = 0\n",
        "            # loop through each doc in the corpus\n",
        "            for doc in corpus:\n",
        "                current_index = 0\n",
        "                doc_len = len(doc)\n",
        "\n",
        "                # grab center word context words\n",
        "                while current_index < doc_len:\n",
        "                    # center word\n",
        "                    word = doc[current_index]\n",
        "                    center_word_idx = self.word_index[word]\n",
        "                    center_word_vector = np.zeros((len(self.vocab), 1))\n",
        "                    center_word_vector[center_word_idx] = 1\n",
        "\n",
        "                    # words in window size\n",
        "                    left_window = max(0, current_index - window_size)\n",
        "                    right_window= min(current_index + window_size, doc_len)\n",
        "                    context_words = doc[left_window:current_index] + doc[current_index+1: right_window]\n",
        "\n",
        "                    # add context words to positive samples (index of context words)\n",
        "                    positive_samples = []\n",
        "                    for cw in context_words:\n",
        "                        cw_idx = self.word_index[cw]\n",
        "                        positive_samples.append(cw_idx)\n",
        "                    \n",
        "                    # grab random 10 neg samples\n",
        "                    negative_samples = list(set(self.negative_sample(10)) - set(positive_samples))\n",
        "\n",
        "                    # samples to be updated\n",
        "                    total_samples = positive_samples + negative_samples\n",
        "\n",
        "                    # label to be predicted of selected words\n",
        "                    label = np.zeros((len(self.vocab), 1))\n",
        "                    label[positive_samples] = 1\n",
        "\n",
        "                    # feed forward word through network\n",
        "                    self.forward(center_word_vector)\n",
        "\n",
        "                    # backprop\n",
        "                    self.backprop(center_word_vector, label, total_samples, center_word_idx)\n",
        "\n",
        "                    # calculate loss\n",
        "                    loss = - np.sum(np.log(self.y[positive_samples])) - np.sum(-self.y[negative_samples])\n",
        "                    \n",
        "                    current_index += 1\n",
        "            \n",
        "            self.train_loss.append((epoch, loss))\n",
        "            if epoch > 1:\n",
        "                if abs(self.train_loss[-2][1] - self.train_loss[-1][1]) < 1e-4:\n",
        "                    print ('Stopping Early')\n",
        "                    break\n",
        "\n",
        "            # print(\"Epoch: \", epoch, \"Loss\", loss)\n",
        "            if epoch % 10 == 0:\n",
        "                print(\"Epoch: \", epoch, \"Loss\", loss)\n",
        "\n",
        "    # make predictions\n",
        "    def predict(self, x, k=5):\n",
        "        # convert word to one-hot-vector\n",
        "        center_word_idx = self.word_index[x.lower()]\n",
        "        center_word_vector = np.zeros((len(self.vocab), 1))\n",
        "        center_word_vector[center_word_idx] = 1\n",
        "\n",
        "        # feed it forward in the network and grab most similar words\n",
        "        words = []\n",
        "        h = np.dot(self.W1.T, center_word_vector)\n",
        "        u = np.dot(self.W2.T, h)\n",
        "        y = self.softmax(u)\n",
        "\n",
        "        # store pred and keep trach of their index\n",
        "        pred = dict(zip(range(len(y)), y))\n",
        "        # sort based on probability of each word to be a context word\n",
        "        pred_sorted = sorted(pred, key=lambda x: pred[x], reverse=True)\n",
        "\n",
        "        # select the top k words\n",
        "        top_context = pred_sorted[:k]\n",
        "        # grab the word using its index from the vocab\n",
        "        for w in top_context:\n",
        "            words.append(self.vocab[w])\n",
        "\n",
        "        return words\n"
      ],
      "metadata": {
        "id": "Yn1Hlsfg-d-B"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "simple_corpus = \"The quick brown fox jumps over the lazy dog.\""
      ],
      "metadata": {
        "id": "HibRwsxRsFCZ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tree_sg = SkipGram()"
      ],
      "metadata": {
        "id": "uaxFJvxQTSRW"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tree_sg.fit(simple_corpus, N=100, window_size=2, epochs=100, learning_rate=.01)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abda00fc-6bf0-48ef-d307-b1b4a1e89cb0",
        "id": "HKLGwZzLTSRW"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:  0 Loss 27.762170661715051871\n",
            "Epoch:  10 Loss 1.637661212819013209\n",
            "Epoch:  20 Loss 0.19866546731102966619\n",
            "Epoch:  30 Loss 0.13702310372069873756\n",
            "Epoch:  40 Loss 0.067367665164215754286\n",
            "Epoch:  50 Loss 0.07927632780390618027\n",
            "Epoch:  60 Loss 0.066287248172391194734\n",
            "Epoch:  70 Loss 0.056254114102344675032\n",
            "Epoch:  80 Loss 0.0417184141723886248\n",
            "Epoch:  90 Loss 0.043487697881388946823\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tree_sg.predict('fox', 2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EkhNZlwRsLHf",
        "outputId": "036d5fe7-c2c3-4faa-b0d7-d73b2f22cfc9"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['jumps', 'brown']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using treebank corpus"
      ],
      "metadata": {
        "id": "ka1iu7EYF3CX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Grab random 500 doc from tree bank corpus \"more docs than used in normal skip gram\""
      ],
      "metadata": {
        "id": "fScpQtigF6M2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('treebank')\n",
        "from nltk.corpus import treebank"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tZb-KGdSvqfi",
        "outputId": "927a809c-51ef-40ac-c403-bf82b8d80890"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/treebank.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(0)\n",
        "corpus = np.random.choice(treebank.sents(), 500)"
      ],
      "metadata": {
        "id": "_qEoIqvQvsYO"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tree_bank_sg = SkipGram()\n",
        "tree_bank_sg.fit(corpus, N=30, window_size=2, epochs=500, learning_rate=.01)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mqrv7iQMvslR",
        "outputId": "376045fd-2e2b-4364-8ac8-9952ec485608"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:  0 Loss 5.7034501385658579685\n",
            "Epoch:  10 Loss 6.6688911679724960655\n",
            "Epoch:  20 Loss 6.1371392216316975063\n",
            "Epoch:  30 Loss 4.027964769061404772\n",
            "Epoch:  40 Loss 3.1897386700563670541\n",
            "Epoch:  50 Loss 2.438827236072422324\n",
            "Epoch:  60 Loss 2.0603206056129379797\n",
            "Epoch:  70 Loss 1.9086652246541145276\n",
            "Epoch:  80 Loss 1.4327058146553509392\n",
            "Epoch:  90 Loss 1.7184273667191716421\n",
            "Epoch:  100 Loss 2.0244380512727827934\n",
            "Epoch:  110 Loss 1.3366981486923037874\n",
            "Epoch:  120 Loss 1.3548533045489924607\n",
            "Epoch:  130 Loss 0.66194677665427943295\n",
            "Epoch:  140 Loss 0.71901751988829729843\n",
            "Epoch:  150 Loss 0.6718347131520239223\n",
            "Epoch:  160 Loss 1.3457234810726876783\n",
            "Epoch:  170 Loss 0.76357242681824916955\n",
            "Epoch:  180 Loss 0.56373279233549101195\n",
            "Epoch:  190 Loss 0.97705589363694978476\n",
            "Epoch:  200 Loss 0.5389898146508087449\n",
            "Epoch:  210 Loss 0.4700505170512265051\n",
            "Epoch:  220 Loss 0.28300066818794673856\n",
            "Epoch:  230 Loss 0.62334299426920901144\n",
            "Epoch:  240 Loss 0.30333818616339749513\n",
            "Epoch:  250 Loss 0.44407441863354011152\n",
            "Epoch:  260 Loss 1.1767863572226504251\n",
            "Epoch:  270 Loss 1.70574504220118747\n",
            "Epoch:  280 Loss 0.5183738042608863751\n",
            "Epoch:  290 Loss 0.52766123720770111236\n",
            "Epoch:  300 Loss 0.2019339465027282137\n",
            "Epoch:  310 Loss 0.20834929096909735627\n",
            "Epoch:  320 Loss 0.28088833708097908225\n",
            "Epoch:  330 Loss 0.22065352458536475642\n",
            "Epoch:  340 Loss 0.34086205855550833112\n",
            "Epoch:  350 Loss 0.24202567782242108469\n",
            "Epoch:  360 Loss 0.1701318736863382044\n",
            "Epoch:  370 Loss 0.23982455882876331444\n",
            "Epoch:  380 Loss 0.27108327556693343342\n",
            "Epoch:  390 Loss 0.2450620200365600089\n",
            "Epoch:  400 Loss 1.1736057395573534227\n",
            "Epoch:  410 Loss 0.3115820427318265221\n",
            "Epoch:  420 Loss 0.54646032654800829557\n",
            "Epoch:  430 Loss 0.19694990522485568107\n",
            "Epoch:  440 Loss 1.1078687708761502365\n",
            "Epoch:  450 Loss 0.3001300385686710638\n",
            "Epoch:  460 Loss 0.2088974937864911567\n",
            "Epoch:  470 Loss 0.16787791152756636273\n",
            "Epoch:  480 Loss 1.1515125666080631716\n",
            "Stopping Early\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# same test used without negative sampling\n",
        "print(np.random.choice(corpus))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88DHTj9BbS7C",
        "outputId": "d984d29f-8d7c-4545-8639-a950c526f6f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['But', 'the', 'strength', 'in', 'heating', 'oil', 'helped', '*-1', 'push', 'up', 'crude', 'oil', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tree_bank_sg.predict('oil')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jcxqFwpfv5x4",
        "outputId": "75fb5d8f-995e-42bf-df7f-4b12ac04af91"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['heating', 'strength', 'crude', 'helped', 'push']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    }
  ]
}