{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CBOW.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejbKT0p4tIVB",
        "outputId": "3abf8c16-e513-4c4b-fbdf-0d12acbcaf33"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.insert(0,'/content/drive/My Drive/python_modules')"
      ],
      "metadata": {
        "id": "Tpu9A-F1tGYk"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "m4180LWKua57"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "-94LFgJE7UGr"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from NLP.Utils import Utils"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CBOW:\n",
        "    def __init__(self):\n",
        "        self.learning_rate = 0.01\n",
        "\n",
        "    # preprocess corpus\n",
        "    def clean_corpus(self, corpus):\n",
        "        if type(corpus[0]) == list:\n",
        "            clean_corpus, word_counts = Utils.clean_docs(corpus)\n",
        "            return clean_corpus, word_counts\n",
        "        else:\n",
        "            prep_corpus = Utils.tokenize(corpus, 'bulk')\n",
        "            clean_corpus, word_counts = Utils.clean_docs(prep_corpus)\n",
        "            return clean_corpus, word_counts\n",
        "        \n",
        "    # convert output to probability\n",
        "    def softmax(self, u):\n",
        "        x = np.exp(u) / np.sum(np.exp(u))\n",
        "        return x\n",
        "    \n",
        "    # initialize words weights\n",
        "    def initialize_weights(self, V, N):\n",
        "        \"\"\"\n",
        "            W1 (hidden layer) = shape(Vocab_length(V), Word_dim(N))\n",
        "            W1 = | d1-w1 d2-w1 ... dN-w1 |\n",
        "                 | d1-w2 d2-w2 ... dN-w2 |\n",
        "                 | d1-wV d2-wV ... dN-wV |\n",
        "\n",
        "            W2 (output layer) = shape(Word_dim(N), Vocab_length(V))\n",
        "            W2 = | d1-w1 d1-w2 ... d1-wV |\n",
        "                 | d2-w1 d2-w2 ... d2-wV |\n",
        "                 | dN-w1 dN-w2 ... dN-wV |\n",
        "        \"\"\"\n",
        "        np.random.seed(0)\n",
        "        self.W1 = np.random.randn(V, N).astype('float128')\n",
        "        self.W2 = np.random.randn(N, V).astype('float128') \n",
        "    \n",
        "    # update weights based on gradient\n",
        "    def update_weights(self, dW2, dW1):\n",
        "        self.W2 = self.W2 - self.learning_rate * dW2\n",
        "        self.W1 = self.W1 - self.learning_rate * dW1\n",
        "\n",
        "    # feed forward\n",
        "    def forward(self, X, predict=False, k=5):\n",
        "        self.x_avg = (np.sum(X, axis=1, keepdims=True) / X.shape[1]).copy()\n",
        "        self.h = np.dot(self.W1.T, self.x_avg)\n",
        "        self.u = np.dot(self.W2.T, self.h)\n",
        "        self.y = self.softmax(self.u)\n",
        "\n",
        "        if predict:\n",
        "            words = []\n",
        "            # store pred and keep trach of their index\n",
        "            pred = dict(zip(range(len(self.y)), self.y))\n",
        "            # sort based on probability of each word to be a context word\n",
        "            pred_sorted = sorted(pred, key=lambda x: pred[x], reverse=True)\n",
        "            # select the top k words\n",
        "            top_context = pred_sorted[:k]\n",
        "            # grab the word using its index from the vocab\n",
        "            for w in top_context:\n",
        "                words.append(self.vocab[w])\n",
        "            return words\n",
        "\n",
        "    # backprop error and calculate gradient\n",
        "    def backprop(self, x, label):\n",
        "        error = self.y - label\n",
        "        dW2 = np.dot(self.h, error.T)\n",
        "        dh = np.dot(self.W2, error)\n",
        "        dW1 = np.dot(self.x_avg, dh.T)\n",
        "        self.update_weights(dW2, dW1)\n",
        "\n",
        "    # train the model\n",
        "    def fit(self, corpus, N=10, window_size=2, epochs=500, learning_rate=.01):\n",
        "        # clean corpus\n",
        "        corpus, word_counts = self.clean_corpus(corpus)\n",
        "        self.cleaned_corpus = corpus\n",
        "        self.vocab = sorted(list(word_counts.keys()))\n",
        "        self.word_index = Utils.vocab_idx(self.vocab)\n",
        "\n",
        "        # initialize parameters\n",
        "        self.initialize_weights(len(self.vocab), N)\n",
        "\n",
        "        if learning_rate is not None:\n",
        "            self.learning_rate = learning_rate\n",
        "        \n",
        "        self.train_loss = []\n",
        "        vocab_len = len(self.vocab)\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            loss = 0\n",
        "            for doc in corpus:\n",
        "                # index to track position in the doc\n",
        "                current_index = 0\n",
        "                \n",
        "                doc_len = len(doc)\n",
        "                # grab center word context words\n",
        "                while current_index < doc_len:\n",
        "                    # center word\n",
        "                    target_word = doc[current_index]\n",
        "\n",
        "                    # words in window size\n",
        "                    left_window = max(0, current_index - window_size)\n",
        "                    right_window= min(current_index + window_size, doc_len)\n",
        "                    context_words = doc[left_window:current_index] + doc[current_index+1: right_window]\n",
        "                    if len(context_words) == 0:\n",
        "                        current_index += 1\n",
        "                        continue\n",
        "                    # context word one-hot-vector for each context word\n",
        "                    input = np.zeros((vocab_len, len(context_words)))\n",
        "\n",
        "                    context_words_idx = [self.word_index[cw] for cw in context_words]\n",
        "                    idx = np.arange(len(context_words_idx))\n",
        "\n",
        "                    input[context_words_idx, idx] = 1\n",
        "\n",
        "                    # convert target word to one-hot-vector\n",
        "                    target_word_idx = self.word_index[target_word]\n",
        "                    target_word_vector = np.zeros((vocab_len, 1))\n",
        "                    target_word_vector[target_word_idx] = 1\n",
        "\n",
        "                    # feed forward word through network\n",
        "                    self.forward(input) \n",
        "\n",
        "                    # backprop\n",
        "                    self.backprop(input, target_word_vector)\n",
        "\n",
        "                    # calculate loss\n",
        "                    loss = -np.log(self.y[target_word_idx])\n",
        "\n",
        "                    # increment index\n",
        "                    current_index += 1 \n",
        "\n",
        "            self.train_loss.append((epoch, loss))\n",
        "            if epoch > 1:\n",
        "                if abs(self.train_loss[-2][1][0] - self.train_loss[-1][1][0]) < 1e-5 \\\n",
        "                or self.train_loss[-2][1][0] < self.train_loss[-1][1][0]:\n",
        "                    print ('Stopping Early')\n",
        "                    break\n",
        "\n",
        "            if epoch % 10 == 0:\n",
        "                print(\"Epoch: \", epoch, \"Loss\", loss)\n",
        "\n",
        "    # make predictions\n",
        "    def predict(self, x, k=5):\n",
        "        cleaned, _ = Utils.clean_docs(x)\n",
        "        # convert context words to one-hot-vectors\n",
        "        context_words_idx = [self.word_index[w.lower()] for w in cleaned[0]]\n",
        "        context_words_vector = np.zeros((len(self.vocab), len(cleaned[0])))\n",
        "\n",
        "        idx = np.arange(len(context_words_idx))\n",
        "        context_words_vector[context_words_idx, idx] = 1\n",
        "\n",
        "        # feed it forward in the network and grab most similar words\n",
        "        return self.forward(context_words_vector, predict=True, k=k)"
      ],
      "metadata": {
        "id": "XklJ8a3mgtKU"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tree bank corpus"
      ],
      "metadata": {
        "id": "bL7MSWhuigM-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('treebank')\n",
        "from nltk.corpus import treebank"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MqMIhmKRbm8p",
        "outputId": "c3014bbb-d93e-46bc-c07b-6f9e5ce3362f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Package treebank is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(0)\n",
        "corpus = np.random.choice(treebank.sents(), 50)"
      ],
      "metadata": {
        "id": "q6g4Zr5da30C"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tree_bank_cbow = CBOW()"
      ],
      "metadata": {
        "id": "kktLjjnfcp3-"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tree_bank_cbow.fit(corpus, epochs=1000, learning_rate=.01)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2eN_7Vx8cto7",
        "outputId": "6163070f-1c53-494e-b12f-375db70abf64"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:  0 Loss [10.94631158]\n",
            "Epoch:  10 Loss [9.39053767]\n",
            "Epoch:  20 Loss [8.26339969]\n",
            "Epoch:  30 Loss [7.35713468]\n",
            "Epoch:  40 Loss [6.57207851]\n",
            "Epoch:  50 Loss [5.85326182]\n",
            "Epoch:  60 Loss [5.16985854]\n",
            "Epoch:  70 Loss [4.50842321]\n",
            "Epoch:  80 Loss [3.86900256]\n",
            "Epoch:  90 Loss [3.26029678]\n",
            "Epoch:  100 Loss [2.69549227]\n",
            "Epoch:  110 Loss [2.19400489]\n",
            "Epoch:  120 Loss [1.78069585]\n",
            "Epoch:  130 Loss [1.47040447]\n",
            "Epoch:  140 Loss [1.25469781]\n",
            "Epoch:  150 Loss [1.10912823]\n",
            "Epoch:  160 Loss [1.00836062]\n",
            "Epoch:  170 Loss [0.93388206]\n",
            "Epoch:  180 Loss [0.87439624]\n",
            "Epoch:  190 Loss [0.82368736]\n",
            "Epoch:  200 Loss [0.77853124]\n",
            "Epoch:  210 Loss [0.73729223]\n",
            "Epoch:  220 Loss [0.69912344]\n",
            "Epoch:  230 Loss [0.66355234]\n",
            "Epoch:  240 Loss [0.63027749]\n",
            "Epoch:  250 Loss [0.59907085]\n",
            "Epoch:  260 Loss [0.56973367]\n",
            "Epoch:  270 Loss [0.5420844]\n",
            "Epoch:  280 Loss [0.51596543]\n",
            "Epoch:  290 Loss [0.491254]\n",
            "Epoch:  300 Loss [0.46786521]\n",
            "Epoch:  310 Loss [0.44574463]\n",
            "Epoch:  320 Loss [0.42485544]\n",
            "Epoch:  330 Loss [0.40516614]\n",
            "Epoch:  340 Loss [0.38664225]\n",
            "Epoch:  350 Loss [0.36924229]\n",
            "Epoch:  360 Loss [0.35291726]\n",
            "Epoch:  370 Loss [0.33761208]\n",
            "Epoch:  380 Loss [0.32326782]\n",
            "Epoch:  390 Loss [0.30982385]\n",
            "Epoch:  400 Loss [0.29721957]\n",
            "Epoch:  410 Loss [0.28539559]\n",
            "Epoch:  420 Loss [0.27429459]\n",
            "Epoch:  430 Loss [0.26386202]\n",
            "Epoch:  440 Loss [0.25404667]\n",
            "Epoch:  450 Loss [0.24480116]\n",
            "Epoch:  460 Loss [0.23608206]\n",
            "Epoch:  470 Loss [0.22784989]\n",
            "Epoch:  480 Loss [0.22006882]\n",
            "Epoch:  490 Loss [0.21270636]\n",
            "Epoch:  500 Loss [0.20573294]\n",
            "Epoch:  510 Loss [0.19912164]\n",
            "Epoch:  520 Loss [0.19284782]\n",
            "Epoch:  530 Loss [0.18688891]\n",
            "Epoch:  540 Loss [0.18122418]\n",
            "Epoch:  550 Loss [0.17583454]\n",
            "Epoch:  560 Loss [0.17070242]\n",
            "Epoch:  570 Loss [0.16581157]\n",
            "Epoch:  580 Loss [0.16114701]\n",
            "Epoch:  590 Loss [0.15669485]\n",
            "Epoch:  600 Loss [0.15244227]\n",
            "Epoch:  610 Loss [0.14837735]\n",
            "Epoch:  620 Loss [0.14448906]\n",
            "Epoch:  630 Loss [0.14076713]\n",
            "Epoch:  640 Loss [0.13720206]\n",
            "Epoch:  650 Loss [0.13378497]\n",
            "Epoch:  660 Loss [0.13050762]\n",
            "Epoch:  670 Loss [0.12736234]\n",
            "Epoch:  680 Loss [0.12434194]\n",
            "Epoch:  690 Loss [0.12143976]\n",
            "Epoch:  700 Loss [0.11864954]\n",
            "Epoch:  710 Loss [0.11596545]\n",
            "Epoch:  720 Loss [0.11338203]\n",
            "Epoch:  730 Loss [0.11089417]\n",
            "Epoch:  740 Loss [0.10849708]\n",
            "Epoch:  750 Loss [0.10618626]\n",
            "Epoch:  760 Loss [0.10395751]\n",
            "Epoch:  770 Loss [0.10180687]\n",
            "Epoch:  780 Loss [0.0997306]\n",
            "Epoch:  790 Loss [0.09772522]\n",
            "Epoch:  800 Loss [0.09578742]\n",
            "Epoch:  810 Loss [0.09391411]\n",
            "Epoch:  820 Loss [0.09210236]\n",
            "Epoch:  830 Loss [0.09034941]\n",
            "Epoch:  840 Loss [0.08865265]\n",
            "Epoch:  850 Loss [0.08700962]\n",
            "Epoch:  860 Loss [0.085418]\n",
            "Epoch:  870 Loss [0.08387558]\n",
            "Epoch:  880 Loss [0.08238029]\n",
            "Epoch:  890 Loss [0.08093016]\n",
            "Epoch:  900 Loss [0.0795233]\n",
            "Epoch:  910 Loss [0.07815797]\n",
            "Epoch:  920 Loss [0.07683246]\n",
            "Epoch:  930 Loss [0.07554519]\n",
            "Epoch:  940 Loss [0.07429465]\n",
            "Epoch:  950 Loss [0.07307938]\n",
            "Epoch:  960 Loss [0.07189803]\n",
            "Epoch:  970 Loss [0.0707493]\n",
            "Epoch:  980 Loss [0.06963193]\n",
            "Epoch:  990 Loss [0.06854476]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = []\n",
        "for  doc in tree_bank_cbow.cleaned_corpus[:10]:\n",
        "    for i in range(2, len(doc) - 2):\n",
        "        context = [doc[i - 2], doc[i - 1], doc[i + 1], doc[i + 2]]\n",
        "        target = doc[i]\n",
        "        data.append((context, target))"
      ],
      "metadata": {
        "id": "mzVeJY4RiqgQ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(20):\n",
        "    idx = np.random.randint(0, len(data)+1)\n",
        "    sent = data[idx]\n",
        "    print('Sentence: ', sent[0], '|Target: ', sent[1])\n",
        "    print('Predicted word:', tree_bank_cbow.predict([sent[0]], 1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53kULUoWjEW1",
        "outputId": "fc4f53f9-8791-44df-8bed-a0220296cf6d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence:  ['trying', 'help', 'unfair', 'testing'] |Target:  kids\n",
            "Predicted word: ['kids']\n",
            "Sentence:  ['closed', 'yesterday', 'york', 'stock'] |Target:  new\n",
            "Predicted word: ['exchange']\n",
            "Sentence:  ['state', 'university', 'professor', 'concluded'] |Target:  education\n",
            "Predicted word: ['education']\n",
            "Sentence:  ['oust', 'mr', 'chairman', 'datapoint'] |Target:  edelman\n",
            "Predicted word: ['edelman']\n",
            "Sentence:  ['nt', 'used', 'similarity', 'actual'] |Target:  classroom\n",
            "Predicted word: ['classroom']\n",
            "Sentence:  ['dallas', 'investor', 'simmons', 'offered'] |Target:  harold\n",
            "Predicted word: ['report']\n",
            "Sentence:  ['st', 'mary', 'ilminster', 'somerset'] |Target:  church\n",
            "Predicted word: ['ilminster']\n",
            "Sentence:  ['trying', 'help', 'unfair', 'testing'] |Target:  kids\n",
            "Predicted word: ['kids']\n",
            "Sentence:  ['nl', 'industries', 'dallas', 'investor'] |Target:  controlled\n",
            "Predicted word: ['controlled']\n",
            "Sentence:  ['schoolteacher', 'william', 'michigan', 'state'] |Target:  mehrens\n",
            "Predicted word: ['mehrens']\n",
            "Sentence:  ['university', 'education', 'concluded', 'study'] |Target:  professor\n",
            "Predicted word: ['professor']\n",
            "Sentence:  ['bells', 'fallen', 'following', 'dustup'] |Target:  silent\n",
            "Predicted word: ['sued']\n",
            "Sentence:  ['either', 'faster', 'execution', 'desirable'] |Target:  cleaner\n",
            "Predicted word: ['cleaner']\n",
            "Sentence:  ['last', 'june', 'test', 'versions'] |Target:  cat\n",
            "Predicted word: ['cat']\n",
            "Sentence:  ['fallen', 'silent', 'dustup', 'church'] |Target:  following\n",
            "Predicted word: ['following']\n",
            "Sentence:  ['fallen', 'silent', 'dustup', 'church'] |Target:  following\n",
            "Predicted word: ['following']\n",
            "Sentence:  ['state', 'university', 'professor', 'concluded'] |Target:  education\n",
            "Predicted word: ['education']\n",
            "Sentence:  ['scoring', 'high', 'materials', 'nt'] |Target:  learning\n",
            "Predicted word: ['learning']\n",
            "Sentence:  ['georgia', 'gulf', 'nl', 'industries'] |Target:  added\n",
            "Predicted word: ['nl']\n",
            "Sentence:  ['fallen', 'silent', 'dustup', 'church'] |Target:  following\n",
            "Predicted word: ['following']\n"
          ]
        }
      ]
    }
  ]
}